#https://www.kaggle.com/datasets/vishakhdapat/customer-segmentation-clustering

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt

# Load dataset
customer_data = pd.read_csv('customer_segmentation.csv')

# Preprocessing
def preprocess_data(data):
    # Drop non-numeric and unnecessary columns if any were mistakenly included again
    if 'Dt_Customer' in data.columns:
        data = data.drop('Dt_Customer', axis=1)
    
    # Encode categorical variables
    label_encoders = {}
    for col in ['Education', 'Marital_Status']:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
    
    # Fill missing values in Income and standardize numerical features excluding 'Response'
    data['Income'].fillna(data['Income'].median(), inplace=True)
    numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.difference(['Response'])
    scaler = StandardScaler()
    data[numerical_features] = scaler.fit_transform(data[numerical_features])
    
    return data, label_encoders

customer_data, label_encoders = preprocess_data(customer_data)

# Features and target
X = customer_data.drop('Response', axis=1)
y = customer_data['Response'].astype(int)

# Coreset selection methods
def no_coreset_selection(X_train, y_train, coreset_size=None):
    return X_train, y_train

def random_sampling_coreset(X_train, y_train, coreset_size):
    indices = np.random.choice(len(X_train), size=coreset_size, replace=False)
    return X_train.iloc[indices], y_train.iloc[indices]

def stratified_sampling_coreset(X_train, y_train, coreset_size):
    X_coreset, _, y_coreset, _ = train_test_split(X_train, y_train, train_size=coreset_size, stratify=y_train, random_state=42)
    return X_coreset, y_coreset

def kmeans_clustering_coreset(X_train, y_train, coreset_size):
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=coreset_size, random_state=42)
    kmeans.fit(X_train)
    centers = pd.DataFrame(kmeans.cluster_centers_, columns=X_train.columns)
    
    from sklearn.neighbors import NearestNeighbors
    nbrs = NearestNeighbors(n_neighbors=1).fit(X_train)
    nearest_indices = nbrs.kneighbors(centers, return_distance=False).flatten()
    y_coreset = y_train.iloc[nearest_indices]
    
    return centers, y_coreset

def uncertainty_sampling_coreset(X_train, y_train, coreset_size):
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_train)[:, 1]
    uncertainty = np.abs(proba - 0.5)
    indices = np.argsort(uncertainty)[:coreset_size]
    return X_train.iloc[indices], y_train.iloc[indices]

def importance_sampling_coreset(X_train, y_train, coreset_size):
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train, y_train)
    proba = model.predict_proba(X_train)[:, 1]
    importance = np.abs(proba - 0.5)
    importance /= importance.sum()
    indices = np.random.choice(len(X_train), size=coreset_size, replace=False, p=importance)
    return X_train.iloc[indices], y_train.iloc[indices]

def reservoir_sampling_coreset(X_train, y_train, coreset_size):
    n = len(X_train)
    indices = list(range(n))
    np.random.shuffle(indices)
    selected_indices = indices[:coreset_size]
    return X_train.iloc[selected_indices], y_train.iloc[selected_indices]

def train_and_evaluate(X_train, y_train, X_test, y_test, coreset_method, coreset_size=None):
    coreset_methods = {
        'none': no_coreset_selection,
        'random': random_sampling_coreset,
        'stratified': stratified_sampling_coreset,
        'kmeans': kmeans_clustering_coreset,
        'uncertainty': uncertainty_sampling_coreset,
        'importance': importance_sampling_coreset,
        'reservoir': reservoir_sampling_coreset
    }
    
    X_coreset, y_coreset = coreset_methods[coreset_method](X_train, y_train, coreset_size)
    
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_coreset, y_coreset)
    
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    cm = confusion_matrix(y_test, y_pred)
    cr = classification_report(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    print(f"\nCoreset Method: {coreset_method}")
    print("Confusion Matrix:")
    print(cm)
    print("Classification Report:")
    print(cr)
    print(f"ROC AUC Score: {roc_auc:.4f}")
    
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    plt.plot(fpr, tpr, label=f'{coreset_method} (AUC = {roc_auc:.4f})')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    #plt.show()
    
    return {'coreset_method': coreset_method, 'roc_auc': roc_auc}

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Evaluate all methods
coreset_methods = ['none', 'random', 'stratified', 'kmeans', 'uncertainty', 'importance', 'reservoir']
coreset_size = 100  # Example size

results = []
for method in coreset_methods:
    results.append(train_and_evaluate(X_train, y_train, X_test, y_test, method, coreset_size))

# Compare results
results_df = pd.DataFrame(results)
print("\nComparison of Coreset Methods:")
print(results_df)
